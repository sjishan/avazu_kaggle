{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV   #Performing grid search\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting Function\n",
    "- sklearn version of the function shuffles the rows, for this case we want to avoid it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(x, y, ratio):\n",
    "    rows_number = y.shape[0]\n",
    "    \n",
    "    bound = int(round(rows_number*ratio))\n",
    "\n",
    "    return x[0:bound,:] , x[bound:,:] , y[0:bound], y[bound:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics Function \n",
    "- numpy and sklearn performance metric functions are fine as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance_metrics(pred_labels, true_labels):\n",
    "\n",
    "    #Positive Cases\n",
    "    PC = np.sum(true_labels == 1) \n",
    "    \n",
    "    #Negative Cases\n",
    "    NC = np.sum(true_labels == 0)\n",
    "    \n",
    "    # True Positive \n",
    "    TP = np.sum(np.logical_and(pred_labels == 1, true_labels == 1))\n",
    " \n",
    "    # True Negative \n",
    "    TN = np.sum(np.logical_and(pred_labels == 0, true_labels == 0))\n",
    " \n",
    "    # False Positive \n",
    "    FP = np.sum(np.logical_and(pred_labels == 1, true_labels == 0))\n",
    " \n",
    "    # False Negative \n",
    "    FN = np.sum(np.logical_and(pred_labels == 0, true_labels == 1))\n",
    "    \n",
    "    #Accuracy\n",
    "    Accuracy = (TP+TN)/float(TP+FP+TN+FN)\n",
    "    \n",
    "    #Precision\n",
    "    Precision = (TP)/float(TP+FP)\n",
    "    \n",
    "    #Recall\n",
    "    Recall = (TP)/float(TP+FN)\n",
    "    \n",
    "    #AUC\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_labels, pred_labels)\n",
    "    AUC = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    print 'Positive Cases: %d \\t True Positive: %d \\t False Positive: %d' % (PC, TP, FP)\n",
    "    print 'Negative Cases: %d \\t True Negative: %d \\t False Negative: %d' % (NC, TN, FN)\n",
    "    print 'Accuracy: %.3f \\t Precision: %.3f \\t Recall: %.3f \\t AUC: %.3f' % (Accuracy, Precision, Recall, AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Feature and Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio = 0.85 # Train - Test set ratio\n",
    "\n",
    "\n",
    "x = np.load('data/features_pca.npy').T #loading the features data\n",
    "y = np.load('data/labels.npy') #loading the label data\n",
    "\n",
    "x_train, x_test, y_train, y_test = data_split(x,y,ratio) #splitting the data     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the model parameters and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "              hidden_layer_sizes=(200,50), # Nodes as of Input Size - 500 - Output Node \n",
    "              activation='relu', # Rectified Linear Unit to avoid vanishing gradient problem\n",
    "              solver='adam', # Optimization part, helps to control the learning the rate\n",
    "              alpha=0.0001, # Regularization term \n",
    "              batch_size='auto', # Leaving it to the sklearn \n",
    "              learning_rate='adaptive', # Slows down as it reaches an optima \n",
    "              learning_rate_init=0.001, \n",
    "              power_t=0.5, \n",
    "              max_iter=200, \n",
    "              shuffle= False, #shuffling is invalid in this case \n",
    "              random_state=None, \n",
    "              tol=0.0001, #iteration termination condition \n",
    "              verbose= True, warm_start=False, \n",
    "              momentum=0.9, # Adam Optimizer parameter\n",
    "              nesterovs_momentum= False, early_stopping=False, #validation_fraction=0.1, \n",
    "              beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67260890\n",
      "Iteration 2, loss = 0.60732580\n",
      "Iteration 3, loss = 0.60316139\n",
      "Iteration 4, loss = 0.60270208\n",
      "Iteration 5, loss = 0.60220475\n",
      "Iteration 6, loss = 0.60211982\n",
      "Iteration 7, loss = 0.60158031\n",
      "Iteration 8, loss = 0.60117278\n",
      "Iteration 9, loss = 0.60083384\n",
      "Iteration 10, loss = 0.60030463\n",
      "Iteration 11, loss = 0.59980824\n",
      "Iteration 12, loss = 0.59986214\n",
      "Iteration 13, loss = 0.59901691\n",
      "Iteration 14, loss = 0.59821113\n",
      "Iteration 15, loss = 0.59745938\n",
      "Iteration 16, loss = 0.59644758\n",
      "Iteration 17, loss = 0.59619850\n",
      "Iteration 18, loss = 0.59540210\n",
      "Iteration 19, loss = 0.59492642\n",
      "Iteration 20, loss = 0.59385275\n",
      "Iteration 21, loss = 0.59312972\n",
      "Iteration 22, loss = 0.59283077\n",
      "Iteration 23, loss = 0.59157873\n",
      "Iteration 24, loss = 0.59089941\n",
      "Iteration 25, loss = 0.59018418\n",
      "Iteration 26, loss = 0.58946580\n",
      "Iteration 27, loss = 0.58840621\n",
      "Iteration 28, loss = 0.58841781\n",
      "Iteration 29, loss = 0.58735775\n",
      "Iteration 30, loss = 0.58652965\n",
      "Iteration 31, loss = 0.58559704\n",
      "Iteration 32, loss = 0.58476599\n",
      "Iteration 33, loss = 0.58420380\n",
      "Iteration 34, loss = 0.58350856\n",
      "Iteration 35, loss = 0.58243585\n",
      "Iteration 36, loss = 0.58161823\n",
      "Iteration 37, loss = 0.58118551\n",
      "Iteration 38, loss = 0.58102457\n",
      "Iteration 39, loss = 0.58012669\n",
      "Iteration 40, loss = 0.57883793\n",
      "Iteration 41, loss = 0.57845781\n",
      "Iteration 42, loss = 0.57710263\n",
      "Iteration 43, loss = 0.57697548\n",
      "Iteration 44, loss = 0.57574143\n",
      "Iteration 45, loss = 0.57527401\n",
      "Iteration 46, loss = 0.57468744\n",
      "Iteration 47, loss = 0.57295120\n",
      "Iteration 48, loss = 0.57268902\n",
      "Iteration 49, loss = 0.57208677\n",
      "Iteration 50, loss = 0.57107250\n",
      "Iteration 51, loss = 0.56964963\n",
      "Iteration 52, loss = 0.56788195\n",
      "Iteration 53, loss = 0.56733710\n",
      "Iteration 54, loss = 0.56597453\n",
      "Iteration 55, loss = 0.56472442\n",
      "Iteration 56, loss = 0.56371026\n",
      "Iteration 57, loss = 0.56250120\n",
      "Iteration 58, loss = 0.56121998\n",
      "Iteration 59, loss = 0.56023269\n",
      "Iteration 60, loss = 0.55917274\n",
      "Iteration 61, loss = 0.55740944\n",
      "Iteration 62, loss = 0.55584601\n",
      "Iteration 63, loss = 0.55502545\n",
      "Iteration 64, loss = 0.55336436\n",
      "Iteration 65, loss = 0.55228131\n",
      "Iteration 66, loss = 0.55069895\n",
      "Iteration 67, loss = 0.54939409\n",
      "Iteration 68, loss = 0.54769806\n",
      "Iteration 69, loss = 0.54741128\n",
      "Iteration 70, loss = 0.54607780\n",
      "Iteration 71, loss = 0.54502171\n",
      "Iteration 72, loss = 0.54281904\n",
      "Iteration 73, loss = 0.54133568\n",
      "Iteration 74, loss = 0.54129263\n",
      "Iteration 75, loss = 0.53865644\n",
      "Iteration 76, loss = 0.53779476\n",
      "Iteration 77, loss = 0.53643432\n",
      "Iteration 78, loss = 0.53497197\n",
      "Iteration 79, loss = 0.53322251\n",
      "Iteration 80, loss = 0.53249190\n",
      "Iteration 81, loss = 0.53226532\n",
      "Iteration 82, loss = 0.52972255\n",
      "Iteration 83, loss = 0.52810946\n",
      "Iteration 84, loss = 0.52765964\n",
      "Iteration 85, loss = 0.52725200\n",
      "Iteration 86, loss = 0.52499124\n",
      "Iteration 87, loss = 0.52376046\n",
      "Iteration 88, loss = 0.52201660\n",
      "Iteration 89, loss = 0.52187384\n",
      "Iteration 90, loss = 0.51998014\n",
      "Iteration 91, loss = 0.52013245\n",
      "Iteration 92, loss = 0.51736929\n",
      "Iteration 93, loss = 0.51762334\n",
      "Iteration 94, loss = 0.51541259\n",
      "Iteration 95, loss = 0.51552456\n",
      "Iteration 96, loss = 0.51427344\n",
      "Iteration 97, loss = 0.51313309\n",
      "Iteration 98, loss = 0.51103811\n",
      "Iteration 99, loss = 0.50994314\n",
      "Iteration 100, loss = 0.50874384\n",
      "Iteration 101, loss = 0.50778101\n",
      "Iteration 102, loss = 0.50793643\n",
      "Iteration 103, loss = 0.50676011\n",
      "Iteration 104, loss = 0.50518306\n",
      "Iteration 105, loss = 0.50496367\n",
      "Iteration 106, loss = 0.50406480\n",
      "Iteration 107, loss = 0.50341123\n",
      "Iteration 108, loss = 0.50283246\n",
      "Iteration 109, loss = 0.50127530\n",
      "Iteration 110, loss = 0.49953856\n",
      "Iteration 111, loss = 0.49926688\n",
      "Iteration 112, loss = 0.49964600\n",
      "Iteration 113, loss = 0.49799876\n",
      "Iteration 114, loss = 0.49772526\n",
      "Iteration 115, loss = 0.49579846\n",
      "Iteration 116, loss = 0.49541436\n",
      "Iteration 117, loss = 0.49628582\n",
      "Iteration 118, loss = 0.49411835\n",
      "Iteration 119, loss = 0.49286723\n",
      "Iteration 120, loss = 0.49266252\n",
      "Iteration 121, loss = 0.49185990\n",
      "Iteration 122, loss = 0.49120936\n",
      "Iteration 123, loss = 0.49008132\n",
      "Iteration 124, loss = 0.49109174\n",
      "Iteration 125, loss = 0.48827751\n",
      "Iteration 126, loss = 0.48774159\n",
      "Iteration 127, loss = 0.48770652\n",
      "Iteration 128, loss = 0.48731791\n",
      "Iteration 129, loss = 0.48731202\n",
      "Iteration 130, loss = 0.48431283\n",
      "Iteration 131, loss = 0.48519973\n",
      "Iteration 132, loss = 0.48455496\n",
      "Iteration 133, loss = 0.48535894\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train) #Training\n",
    "y_pred = mlp.predict(x_test) #Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Cases: 6571 \t True Positive: 4670 \t False Positive: 2512\n",
      "Negative Cases: 5351 \t True Negative: 2839 \t False Negative: 1901\n",
      "Accuracy: 0.630 \t Precision: 0.650 \t Recall: 0.711 \t AUC: 0.621\n"
     ]
    }
   ],
   "source": [
    "performance_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
